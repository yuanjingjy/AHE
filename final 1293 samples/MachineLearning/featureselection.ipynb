{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "基于\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.去掉变化最小的特征\n",
    "说明：如果95%的实例的该特征的取值都是1，那就认为这个特征作用不大，如果100%都是1，该特征没意义。\n",
    "应用条件：当特征是离散型变量时才可以使用该方法，对于连续型需要先离散化\n",
    "优缺点：实际上不太会有95%以上都取某个值的特征存在，因此该方法简单但不好用，可以作为特征选择的预处理，先去掉那些取值变化小的特征，然后用其他特征选择方法进行进一步的特征选择。\n",
    "例：去除有80%以上都是0或都是1的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n       [1, 0],\n       [0, 0],\n       [1, 1],\n       [1, 0],\n       [1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.单变量特征选择\n",
    "单变量特征选择能够对每一个特征进行测试，衡之间量该特征和响应变量之间的关系，根据得分去除特征。\n",
    "2.1Pearson相关系数\n",
    "该方法衡量的是变量之间的线性相关性，结果的取值范围为[-1,1],-1表示完全的负相关，+1表示完全的正相关，0表示没有线性相关。scipy的pearsonr方法能够计算相关系数和p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower noise (0.71824836862138408, 7.3240173129983507e-49)\nHigher nois (0.057964292079338155, 0.31700993885324752)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import  pearsonr\n",
    "\n",
    "np.random.seed(0)\n",
    "size=300\n",
    "x=np.random.normal(0,1,size)\n",
    "print(\"Lower noise\",pearsonr(x,x+np.random.normal(0,1,size)))\n",
    "print(\"Higher nois\",pearsonr(x,x+np.random.normal(0,10,size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明：该例中比较了变量加入噪声前后的差异，当变量较小时，相关性强。\n",
    "sklearn提供的f_regression方法能够批量计算特征的p-value。（sklearn的pipeline）\n",
    "pearson相关系数的缺点：作为特征排序机制，只对线性关系敏感，如果关系是非线性的，即使两个变量具有一一对应的关系，pearson相关性也可能接近0.\n",
    "\n",
    "注：如果仅仅根据相关系数这个值来判断的话，有时可能有很，最好将数据可视化进行检验。\n",
    "\n",
    "2.2互信息和最大信息系数\n",
    "用互信息直接进行特征选择的不便之处：1.它不属于度量方式，也没有办法归一化，在不同的数据集上的结果无法做比较；2.对于连续变量的计算不是很方便（X，y都是集合，都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。\n",
    "最大信息系数克服了这两个问题，它首先寻找一种最优的离散化方法，然后把互信息取值转换成一种度量方式，取值在[0,1]上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from minepy import MINE\n",
    "\n",
    "m = MINE()\n",
    "x = np.random.uniform(-1, 1, 10000)\n",
    "m.compute_score(x, x**2)\n",
    "print( m.mic())\n",
    "MIC的统计能力遭到了一些质疑，当零假设不成立时，MIC的统计就会受到影响。\n",
    "在有的数据集上不存在这个问题，但有的数据集上就存在这个问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 距离相关系数 (Distance correlation)\n",
    "距离相关系数是为了克服Pearson相关系数的弱点而生的。在x和x^2这个例子中，即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0，那么我们就可以说这两个变量是独立的。\n",
    "R的energy包里提供了距离相关系数的实现，另外这是Python gist的实现。\n",
    "#R-code\n",
    "> x = runif (1000, -1, 1)\n",
    "> dcor(x, x**2)\n",
    "[1] 0.4943864\n",
    "\n",
    "尽管有MIC和距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。第一、Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。第二、Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。\n",
    "\n",
    "2.4 基于学习模型的特征排序 (Model based ranking)\n",
    "这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。其实Pearson相关系数等价于线性回归里的标准化回归系数。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b67768eb7ecb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m      score = cross_val_score(rf, X[:, i:i+1], Y, scoring=\"r2\",\n\u001b[0;32m     15\u001b[0m                               cv=ShuffleSplit(len(X), 3, .3))\n\u001b[1;32m---> 16\u001b[1;33m      \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score, ShuffleSplit\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Load boston housing dataset as an example\n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=20, max_depth=4)\n",
    "scores = []\n",
    "for i in range(X.shape[1]):\n",
    "     score = cross_val_score(rf, X[:, i:i+1], Y, scoring=\"r2\",\n",
    "                              cv=ShuffleSplit(len(X), 3, .3))\n",
    "     scores.append((round(np.mean(score), 3), names[i]))\n",
    "print (sorted(scores, reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.线性模型和正则化\n",
    "线性模型：用线性模型的系数来选择特征变量，重要的模型在特征中对应的系数比较大，在噪音不多的数据上，或者是数据量远远大于特征数的数据上，如果特征之间相对来说是比较独立的，那么即便是运用最简单的线性回归模型也一样能取得非常好的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model: 0.984*x0+1.995*x1+-0.041*x2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import  LinearRegression\n",
    "import  numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "size=5000\n",
    "#A dataset with 3 features\n",
    "x=np.random.normal(0,1,(size,3))\n",
    "y=x[:,0]+2*x[:,1]+np.random.normal(0,2,size)\n",
    "lr=LinearRegression()\n",
    "lr.fit(x,y)\n",
    "\n",
    "#A helper method for pretty-printing linear models\n",
    "def pretty_print_linear(coefs,names=None,sort=False):\n",
    "    if names==None:\n",
    "        names=[\"x%s\" % x for x in range(len(coefs))]\n",
    "    lst=zip(coefs,names)\n",
    "    if sort:\n",
    "        lst=sorted(lst,key=lambda  x:-np.abs(x[0]))\n",
    "    return \"+\".join(\"%s*%s\" %(round(coef,3),name)\n",
    "                    for coef, name in lst)\n",
    "print(\"Linear model:\",pretty_print_linear(lr.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 L1正则化/Lasso\n",
    "L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0.因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特征使得L1正则化成为一种很好的特征选择方法。Skearn为线性回归提供了Lasso，为分类提供了L1逻辑回归。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso model -3.707*LSTAT+2.992*RM+-1.757*PTRATIO+-1.081*DIS+-0.7*NOX+0.631*B+0.54*CHAS+-0.236*CRIM+0.081*ZN+-0.0*INDUS+-0.0*AGE+0.0*RAD+-0.0*TAX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:14: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import  Lasso\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "from  sklearn.datasets import  load_boston\n",
    "\n",
    "boston=load_boston()\n",
    "scaler=StandardScaler()\n",
    "x=scaler.fit_transform(boston[\"data\"])\n",
    "y=boston[\"target\"]\n",
    "names=boston[\"feature_names\"]\n",
    "lasso=Lasso(alpha=.3)\n",
    "lasso.fit(x,y)\n",
    "print(\"Lasso model\",pretty_print_linear(lasso.coef_,names,sort=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 L2正则化/Ridge regression\n",
    "L2正则化将系数向量的L2范数添加到损失函数中，由于L2惩罚项中的系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是L2正则化会让系数的取值变得平均。对于"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 0\nLinear model: 0.728*x0+2.309*x1+-0.082*x2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge model 0.938*x0+1.059*x1+0.877*x2\nRandom seed 1\nLinear model: 1.152*x0+2.366*x1+-0.599*x2\nRidge model 0.984*x0+1.068*x1+0.759*x2\nRandom seed 2\nLinear model: 0.697*x0+0.322*x1+2.086*x2\nRidge model 0.972*x0+0.943*x1+1.085*x2\nRandom seed 3\nLinear model: 0.287*x0+1.254*x1+1.491*x2\nRidge model 0.919*x0+1.005*x1+1.033*x2\nRandom seed 4\nLinear model: 0.187*x0+0.772*x1+2.189*x2\nRidge model 0.964*x0+0.982*x1+1.098*x2\nRandom seed 5\nLinear model: -1.291*x0+1.591*x1+2.747*x2\nRidge model 0.758*x0+1.011*x1+1.139*x2\nRandom seed 6\nLinear model: 1.199*x0+-0.031*x1+1.915*x2\nRidge model 1.016*x0+0.89*x1+1.091*x2\nRandom seed 7\nLinear model: 1.474*x0+1.762*x1+-0.151*x2\nRidge model 1.018*x0+1.039*x1+0.901*x2\nRandom seed 8\nLinear model: 0.084*x0+1.88*x1+1.107*x2\nRidge model 0.907*x0+1.071*x1+1.008*x2\nRandom seed 9\nLinear model: 0.714*x0+0.776*x1+1.364*x2\nRidge model 0.896*x0+0.903*x1+0.98*x2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import  Ridge\n",
    "from sklearn.metrics import  r2_score\n",
    "size=100\n",
    "#We run the method 10 times with different random seeds\n",
    "for i in range(10):\n",
    "    print(\"Random seed %s\" %i)\n",
    "    np.random.seed(seed=i)\n",
    "    x_seed=np.random.normal(0,1,size)\n",
    "    x1=x_seed+np.random.normal(0,.1,size)\n",
    "    x2=x_seed+np.random.normal(0,.1,size)\n",
    "    x3=x_seed+np.random.normal(0,.1,size)\n",
    "    y=x1+x2+x3+np.random.normal(0,1,size)\n",
    "    x=np.array([x1,x2,x3]).T\n",
    "    \n",
    "    lr=LinearRegression()\n",
    "    lr.fit(x,y)\n",
    "    print(\"Linear model:\",pretty_print_linear(lr.coef_))\n",
    "    \n",
    "    ridge=Ridge(alpha=10)\n",
    "    ridge.fit(x,y)\n",
    "    print(\"Ridge model\", pretty_print_linear(ridge.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.随机森林 \n",
    "随机森林具有准确率高，鲁棒性好易于使用等优点，是目前最流行的机器学习算法之一。随机森林提供了两种特征选择方法：mean decrease imputity和mean decrease accuracy\n",
    "4.1 平均不纯度减少（mean decr impurity）\n",
    "随机森林由多个决策树构成。决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。利用不纯度可以确定节点（最优条件），对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。\n",
    "下面的例子是sklearn中基于随机森林的特征重要度的度量方法：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their scores:\n[(0.4128, 'RM'), (0.38140000000000002, 'LSTAT'), (0.0877, 'DIS'), (0.031600000000000003, 'CRIM'), (0.021399999999999999, 'NOX'), (0.0212, 'PTRATIO'), (0.0117, 'AGE'), (0.010699999999999999, 'B'), (0.0097999999999999997, 'TAX'), (0.0071000000000000004, 'INDUS'), (0.0028, 'RAD'), (0.001, 'ZN'), (0.00080000000000000004, 'CHAS')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import  load_boston\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "import  numpy as np\n",
    "\n",
    "#Load boston housing dataset as an example \n",
    "boston =load_boston()\n",
    "x=boston[\"data\"]\n",
    "y=boston[\"target\"]\n",
    "names=boston[\"feature_names\"]\n",
    "rf=RandomForestRegressor()\n",
    "rf.fit(x,y)\n",
    "print(\"Features sorted by their scores:\")\n",
    "print(sorted(zip(map(lambda  x:round(x,4),rf.feature_importances_),names),reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for X0, X1, X2: <map object at 0x00000270EF5AFA58>\n"
     ]
    }
   ],
   "source": [
    "size=1000\n",
    "np.random.seed(seed=10)\n",
    "x_seed=np.random.normal(0,1,size)\n",
    "x0=x_seed+np.random.normal(0,.1,size)\n",
    "x1=x_seed+np.random.normal(0,.1,size)\n",
    "x2=x_seed+np.random.normal(0,.1,size)\n",
    "x=np.array([x0,x1,x2]).T\n",
    "y=x0+x1+x2\n",
    "\n",
    "rf=RandomForestRegressor(n_estimators=20,max_features=2)\n",
    "rf.fit(x,y)\n",
    "print( \"Scores for X0, X1, X2:\", map(lambda x:round (x,3),\n",
    "                                    rf.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores for X0, X1, X2: [0.278, 0.66, 0.062]\n",
    "当计算特征重要性时，可以看到X1的重要度比X2的重要度要高出10倍，但实际上他们真正的重要度是一样的。尽管数据量已经很大且没有噪音，且用了20棵树来做随机选择，但这个问题还是会存在。\n",
    "需要注意的一点是，关联特征的打分存在不稳定的现象，这不仅仅是随机森林特有的，大多数基于模型的特征选择方法都存在这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature sorted by their score:\n[(0.7611, 'LSTAT'), (0.55979999999999996, 'RM'), (0.093899999999999997, 'DIS'), (0.041799999999999997, 'NOX'), (0.038899999999999997, 'CRIM'), (0.019400000000000001, 'PTRATIO'), (0.0154, 'TAX'), (0.0114, 'AGE'), (0.0054999999999999997, 'B'), (0.0051999999999999998, 'INDUS'), (0.0041000000000000003, 'RAD'), (0.00029999999999999997, 'ZN'), (0.00020000000000000001, 'CHAS')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import  ShuffleSplit\n",
    "from sklearn.metrics import  r2_score\n",
    "from  collections import  defaultdict\n",
    "x=boston[\"data\"]\n",
    "y=boston[\"target\"]\n",
    "\n",
    "rf=RandomForestRegressor()\n",
    "scores=defaultdict(list)\n",
    "\n",
    "#crossvalidate the scores on a number of different random splits of the data\n",
    "for train_idx,test_idx in ShuffleSplit(len(X),100,.3):\n",
    "    x_train,x_test=x[train_idx],x[test_idx]\n",
    "    y_train,y_test=y[train_idx],y[test_idx]\n",
    "    r=rf.fit(x_train,y_train)\n",
    "    acc=r2_score(y_test,rf.predict(x_test))\n",
    "    for i in range(x.shape[1]):\n",
    "            x_t=x_test.copy()\n",
    "            np.random.shuffle(x_t[:,i])\n",
    "            shuff_acc=r2_score(y_test,rf.predict(x_t))\n",
    "            scores[names[i]].append((acc-shuff_acc)/acc)\n",
    "print(\"Feature sorted by their score:\")\n",
    "print(sorted([(round(np.mean(score), 4), feat) for\n",
    "            feat, score in scores.items()], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子当中，LSTAT和RM这两个特征对模型的性能有着很大的影响，打乱这两个特征的特征值使得模型的性能下降了73%和57%。注意，尽管这些我们是在所有特征上进行了训练得到了模型，然后才得到了每个特征的重要性测试，这并不意味着我们扔掉某个或者某些重要特征后模型的性能就一定会下降很多，因为即便某个特征删掉之后，其关联特征一样可以发挥作用，让模型性能基本上不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their score:\n[(1.0, 'RM'), (1.0, 'PTRATIO'), (1.0, 'LSTAT'), (0.62, 'B'), (0.59999999999999998, 'CHAS'), (0.40500000000000003, 'TAX'), (0.38, 'CRIM'), (0.22500000000000001, 'NOX'), (0.215, 'DIS'), (0.11, 'INDUS'), (0.044999999999999998, 'ZN'), (0.025000000000000001, 'RAD'), (0.02, 'AGE')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import  RandomizedLasso\n",
    "from sklearn.datasets import load_boston\n",
    "boston=load_boston()\n",
    "#using the boston housing data\n",
    "#Data gets scaled autoamtically by sklearn's implementation\n",
    "x=boston[\"data\"]\n",
    "y=boston[\"target\"]\n",
    "names=boston[\"feature_names\"]\n",
    "\n",
    "rlasso=RandomizedLasso(alpha=0.025)\n",
    "rlasso.fit(x,y)\n",
    "\n",
    "print(\"Features sorted by their score:\")\n",
    "print(sorted(zip(map(lambda  x: round(x,4),rlasso.scores_),names),reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上边这个例子当中，最高的3个特征得分是1.0，这表示他们总会被选作有用的特征（当然，得分会收到正则化参数alpha的影响，但是sklearn的随机lasso能够自动选择最优的alpha）。接下来的几个特征得分就开始下降，但是下降的不是特别急剧，这跟纯lasso的方法和随机森林的结果不一样。能够看出稳定性选择对于克服过拟合和对数据理解来说都是有帮助的：总的来说，好的特征不会因为有相似的特征、关联特征而得分为0，这跟Lasso是不同的。对于特征选择任务，在许多数据集和环境下，稳定性选择往往是性能最好的方法之一。\n",
    "5.2 递归特征消除 Recursive feature elimination (RFE)\n",
    "递归特征消除的主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征放到一遍，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。\n",
    "RFE的稳定性很大程度上取决于在迭代的时候底层用哪种模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。\n",
    "Sklearn提供了RFE包，可以用于特征消除，还提供了RFECV，可以通过交叉验证来对的特征进行排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features sorted by their rank:\n[(1, 'NOX'), (2, 'RM'), (3, 'CHAS'), (4, 'PTRATIO'), (5, 'DIS'), (6, 'LSTAT'), (7, 'RAD'), (8, 'CRIM'), (9, 'INDUS'), (10, 'ZN'), (11, 'TAX'), (12, 'B'), (13, 'AGE')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston[\"data\"]\n",
    "Y = boston[\"target\"]\n",
    "names = boston[\"feature_names\"]\n",
    "\n",
    "#use linear regression as the model\n",
    "lr = LinearRegression()\n",
    "#rank all features, i.e continue the elimination until the last one\n",
    "rfe = RFE(lr, n_features_to_select=1)\n",
    "rfe.fit(X,Y)\n",
    "\n",
    "print(\"Features sorted by their rank:\")\n",
    "print (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.285402218862249"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import  numpy as np\n",
    "import  math\n",
    "math.log(78,2)\n",
    "#np.linspace(1,10,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
